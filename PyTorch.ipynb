{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2322, 0.3862, 0.2930, 0.0886]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4),\n",
    "  nn.Softmax(dim = -1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help to determine loss function of predication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(1), num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "# nn.CrossEntropyLoss could also be done as from torch.nn import CrossEntropyLoss, then criterion = CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learing rate\n",
    "lr = 0.001\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = model[0].weight.grad\n",
    "grads1 = model[1].weight.grad\n",
    "grads2 = model[2].weight.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - lr * grads0\n",
    "weight1 = weight1 - lr * grads1\n",
    "weight2 = weight2 - lr * grads2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change step size to find global min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "pred = model(sample)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rectified linear unit (ReLU): It overcomes the training problems linked with the sigmoid function you learned, such as the vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all negative numbers will be set to 0. For positive integers the ReLU returns itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is leaky which mean that negative numbers will not be set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu_pytorch = nn.LeakyReLU(negative_slope = 0.05)\n",
    "\n",
    "x = torch.tensor(-3.0)\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".numel() = counting number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(8, 4),                       \n",
    "                      nn.Linear(4, 2))\n",
    "\n",
    "total = 0\n",
    "for parameter in model.parameters():    \n",
    "    total += parameter.numel()\n",
    "print(total)\n",
    "\n",
    "# nn.Linear(8,4) has 4 neuron and each neuron  has 8+1 parameters (the + 1 is related to the bias layer) \n",
    "# nn.Linear(4,2) has 2 neuron and each neuron  has 4+1 parameters (the + 1 is related to the bias layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network with exactly three linear layers and less than 120 parameters, \n",
    "# which takes n_features as inputs and outputs n_classes.\n",
    "\n",
    "def calculate_capacity(model):\n",
    "  total = 0\n",
    "  for p in model.parameters():\n",
    "    total += p.numel()\n",
    "  return total\n",
    "\n",
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with less than 120 parameters\n",
    "model_less = nn.Sequential(\n",
    "    nn.Linear(n_features, 1),\n",
    "    nn.Linear(1, 1),\n",
    "    nn.Linear(1, n_classes)\n",
    ")\n",
    "\n",
    "# Create a neural network with more than 120 parameters\n",
    "model_greater = nn.Sequential(\n",
    "    nn.Linear(n_features, 4),\n",
    "    nn.Linear(4, 8),\n",
    "    nn.Linear(8, 9),\n",
    "    nn.Linear(9, n_classes)\n",
    ")\n",
    "\n",
    "output_less = model_less(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model_less))\n",
    "\n",
    "output_greater = model_greater(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model_greater))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochasitc gradient descent optimizier optim.SGD(model.parameter(), lr = 0.01, momentum = 0.95)\n",
    "\n",
    "- lr = controles the step size \n",
    "    - too small = not making large enough movement to minimum\n",
    "    - too large = never finding minimum because covering too large of an area\n",
    "    - A learning rate around 0.08 - 0.09 gets you closest to the global minimum\n",
    "    \n",
    "- momentum = helps to overcome getting stuck in a local minimum (the inertia of the optimizer)\n",
    "\n",
    "\n",
    "- Momentum and learning rate are critical to the training of your neural network. A good rule of thumb is to start with a learning rate of 0.001 and a momentum of 0.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning = A type of transfer learning\n",
    "- Smaller learning rate\n",
    "- Not every layer is trained (we freeze some of them)\n",
    "- Rule of thumb: freeze early layers of network and fine-tune layers closer to output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Sequential(nn.Linear(64, 128),  \n",
    "                      nn.Linear(128, 256))\n",
    "for name, param in model.named_parameters():\n",
    "    if name == '0.weight':        \n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():    \n",
    "  \n",
    "    # Check if the parameters belong to the first layer\n",
    "    if name == '0.weight' or name == '0.bias':\n",
    "      \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False\n",
    "  \n",
    "    # Check if the parameters belong to the second layer\n",
    "    if name == '1.weight' or name == '1.bias':\n",
    "      \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform initalization \n",
    "layer0 = nn.Linear(16, 32)\n",
    "layer1 = nn.Linear(32, 64)\n",
    "\n",
    "# Use uniform initialization for layer0 and layer1 weights\n",
    "nn.init.uniform_(layer0.weight)\n",
    "nn.init.uniform_(layer1.weight)\n",
    "\n",
    "model = nn.Sequential(layer0, layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorDataset is great to use when your dataset can be loaded from NumPy arrays (or converted to NumPy arrays). However, sometimes you need to code a custom dataset class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "np_features = np.array(np.random.rand(12, 8))\n",
    "np_target = np.array(np.random.rand(12, 1))\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "torch_features = torch.tensor(np_features)\n",
    "torch_target = torch.tensor(np_target)\n",
    "\n",
    "# Create a TensorDataset from two tensors\n",
    "dataset = TensorDataset(torch_features, torch_target)\n",
    "\n",
    "# Return the last element of this dataset\n",
    "print(dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the different columns into two PyTorch tensors\n",
    "features = torch.tensor(np.array(\n",
    "  dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']])).float()\n",
    "target = torch.tensor(np.array(\n",
    "  dataframe['Potability'])).float()\n",
    "\n",
    "# Create a dataset from the two generated tensors\n",
    "dataset = TensorDataset(features, target)\n",
    "\n",
    "# Create a dataloader using the above dataset\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
    "x, y = next(iter(dataloader))\n",
    "\n",
    "# Create a model using the nn.Sequential API\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(4, 2), # 4 = num cols in features\n",
    "  nn.Linear(2,1) # 1 = num cols in target\n",
    ")\n",
    "output = model(features)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "validation_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "  \n",
    "  for data in validationloader:\n",
    "    \n",
    "      outputs = model(data[0])\n",
    "      loss = criterion(outputs, data[1])\n",
    "      \n",
    "      # Sum the current loss to the validation_loss variable\n",
    "      validation_loss += loss.item()\n",
    "      \n",
    "# Calculate the mean loss value\n",
    "validation_loss_epoch = validation_loss/len(validationloader)\n",
    "print(validation_loss_epoch)\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect overfitting when training model is doing better or worse than accuracy line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "# Create accuracy metric using torch metrics\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "for data in dataloader:\n",
    "    features, labels = data\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Calculate accuracy over the batch\n",
    "    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n",
    "    \n",
    "# Calculate accuracy over the whole epoch\n",
    "acc = metric.compute()\n",
    "\n",
    "# Reset the metric for the next epoch \n",
    "metric.reset()\n",
    "plot_errors(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay \n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search is a great way to fine-tune your hyperparameters. Upper and lower bounds should be carefully chosen to not waste computational power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "values = []\n",
    "for idx in range(10):\n",
    "    # Randomly sample a learning rate factor between 0.01 and 0.0001\n",
    "    factor = np.random.uniform(2, 4)\n",
    "    lr = 10 ** -factor\n",
    "    \n",
    "    # Randomly select a momentum between 0.85 and 0.99\n",
    "    momentum = np.random.uniform(0.85, 0.99)\n",
    "    \n",
    "    values.append((lr, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def birthdayCakeCandles(candles):\n",
    "    unique_dict = {}\n",
    "\n",
    "    for candle in candles:\n",
    "        # unique_dict.get(candle, 0): This part retrieves the\n",
    "        # current value associated with the key candle in unique_dict.\n",
    "        # If the key is not present, it returns the default value 0\n",
    "        unique_dict[candle] = unique_dict.get(candle, 0) + 1\n",
    "\n",
    "    max_value = max(unique_dict.values())\n",
    "    print(max_value)\n",
    "\n",
    "array = [3, 2, 1, 3]\n",
    "birthdayCakeCandles(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final x: 0.4999992251396179\n",
      "Final y: 0.062499988824129105\n",
      "Minimum value of f(x, y): 1.718750238418579\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define the function f(x, y)\n",
    "def f(x, y):\n",
    "    return (x + 1)**2 + 8*y**2 - 3*x - y + 1\n",
    "\n",
    "# Initialize variables x and y (starting point for gradient descent)\n",
    "x = Variable(torch.tensor(0.0), requires_grad=True)\n",
    "y = Variable(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "# Set learning rate and number of iterations\n",
    "lr = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Gradient Descent loop\n",
    "for i in range(num_iterations):\n",
    "    # Compute the function value\n",
    "    loss = f(x, y)\n",
    "    \n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update variables using gradient descent\n",
    "    x.data -= lr * x.grad\n",
    "    y.data -= lr * y.grad\n",
    "    \n",
    "    # Zero the gradients to prevent accumulation in the next iteration\n",
    "    x.grad.zero_()\n",
    "    y.grad.zero_()\n",
    "\n",
    "# Print the final values of x and y after gradient descent\n",
    "print(\"Final x:\", x.item())\n",
    "print(\"Final y:\", y.item())\n",
    "\n",
    "# Print the minimum value of the function after gradient descent\n",
    "print(\"Minimum value of f(x, y):\", f(x, y).item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
